{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d733e1a-fff5-4739-b702-cc4af0a69d41",
   "metadata": {},
   "source": [
    "# How to build a simple RAG LLM App with LangChain\n",
    "A Retrieval Augmented Generation (RAG) app is a combination of searching for information and generating new content, all rolled into one application. It's like a smart assistant that not only finds the information you need but also uses that information to create new, useful responses. Here’s how it works, explained in simple terms:\n",
    "\n",
    "#### How a RAG App Works\n",
    "1. **Question or Query**: You start by asking the RAG app a question or giving it a topic you need information about.\n",
    "2. **Retrieval**: The app uses a **retriever** to go through a large document to find related information. This could be articles, books, or other relevant documents. Think of this step like finding all the ingredients you need for a recipe.\n",
    "3. **Augmented Generation**: Once the app has the information, it uses an LLM to generate the response. This generator takes all the information the retriever found and uses it to craft a detailed, accurate answer or content that directly addresses your question or topic.\n",
    "4. **Output**: Finally, the app presents you with a response that isn’t just copied from somewhere else but is a freshly generated piece of content based on the information it retrieved.\n",
    "\n",
    "#### Why It’s Useful\n",
    "- **Accuracy and Relevance**: Because the RAG app pulls information from relevant sources before generating a response, the answers you get are usually more accurate and relevant.\n",
    "- **Efficiency**: It combines the steps of searching for information and creating content, making it a powerful tool for quickly producing high-quality responses.\n",
    "- **Versatility**: This type of app can be used for a wide range of applications, from helping students with their homework to assisting researchers in summarizing scientific articles.\n",
    "\n",
    "#### RAG apps save you money\n",
    "Retrieval Augmented Generation (RAG) apps are particularly good for saving costs associated with large language models (LLMs).\n",
    "\n",
    "1. **Efficient Use of Resources**:\n",
    "   - **Selective Querying**: RAG apps first use a retriever to find relevant information before using a large language model to generate a response. This means the LLM is only called upon when necessary, after the relevant information has already been located. Instead of the LLM processing every aspect of a query from scratch, it focuses on integrating and enhancing the retrieved information to formulate a response.\n",
    "   - **Reduced Computational Load**: By leveraging retrieved content, the LLM performs less computation per query. This is because part of the work (finding relevant information) is already done, so the model just needs to synthesize this information rather than generating content from zero context, which is more resource-intensive.\n",
    "\n",
    "In essence, RAG apps help save on LLM costs by making sure that the expensive resources (like computing power and time) associated with running these large models are used judiciously and only where they provide the most value, such as in the generation of precise and contextually informed responses, rather than in the preliminary gathering of information.\n",
    "\n",
    "#### RAG apps and the privacy of your data\n",
    "RAG apps involve two main steps: retrieving information from documents and then using that information to generate responses. Whether these apps maintain the privacy of the documents they access largely depends on how they are designed and what data they handle. Here's a simple breakdown:\n",
    "\n",
    "#### RAG Apps and Data Privacy\n",
    "RAG is commonly deployed to solve large language model shortcomings, including hallucinations and short context windows. But RAG also helps us build privacy-aware apps.\n",
    "\n",
    "When using RAG, data is provided as context to an LLM only at generation time, but the data does not need to be used for training AI models. This means your data is not stored in the models themselves as knowledge — it’s only shown to LLMs when we ask for responses.\n",
    "\n",
    "Instead of storing your data permanently in the AI model, RAG uses the data only when needed to generate a response. After the response is generated, the data isn't stored or remembered by the LLM.\n",
    "\n",
    "In a production-level RAG Application, successfully building privacy-aware solution requires considering and classifying the data you plan to store upfront, and taking advanced steps to keep your sensible data safe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48386f20-c929-48a9-8720-0953fcd67dd0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ee060-21f2-4e01-b283-1fd656dac1e9",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 001-simple-rag.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1309f80-10e7-4f86-bb65-488897fb5ba9",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a63ff6-99ff-4629-b965-547d12a99ba6",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **001-simple-rag**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6605f3d-63a1-4267-9c85-93c0174af4e9",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4136ad-0dc8-4032-814e-f42a9e39f655",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2576e5d-39c9-490c-a4dc-9b8b5adb75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992ec4a9-aa01-4e44-aeb9-b9a1f3aa9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01d18b-f9f0-427b-a9dc-3d1885160578",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a725bb88-271a-4a48-9f1f-881a9a0f8289",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed746499-d1b8-41e5-b131-270cf5fa229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2af3ef-c2c7-445f-92bd-a29c68abce25",
   "metadata": {},
   "source": [
    "## Connect with an LLM and start a conversation with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff44c5-1524-4413-8e94-c653a8b1957b",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa7337f-3d60-4ede-bdf8-aa7a5cffec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3551e-95ca-41a1-8810-89c495bf93ab",
   "metadata": {},
   "source": [
    "* For this project, we will use OpenAI's gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9afcbc7d-a816-41e3-925f-850883f5770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913fc8c-254f-410d-aa8f-35eb0898855e",
   "metadata": {},
   "source": [
    "#### Track the operation in LangSmith\n",
    "* [Open LangSmith here](smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ef368-6d2b-4097-88c7-edb26d294df6",
   "metadata": {},
   "source": [
    "## Simple RAG App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331814d-bead-4132-9ebe-792e2fb55cbe",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following packages because they are already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1335eda-9f6e-4388-b228-36c464900f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-chroma langchain-community langchain-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7929b56-bed5-47ec-868a-7e452a50f2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article discusses the concept of creating something people want and not worrying about the business model initially. It suggests that this approach could lead to a surprising discovery of a charity-like business model. Examples like Craigslist running successfully with a charity-like approach are provided to support this idea.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "#from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "loader = TextLoader(\"./data/be-good.txt\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#the following line is not compatible with python 3.11.4\n",
    "#to install langchain-hub, you will have to use python 3.12.2 or superior\n",
    "#prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "#to keep using python 3.11.4, we will paste the prompt from the hub\n",
    "prompt  = ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is this article about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37639757-c21b-44be-9e4f-c654d8222f51",
   "metadata": {},
   "source": [
    "#### The format_docs() function\n",
    "The  function `format_docs` takes the list of docs got by the retriever and processes it to format the content into a single string. Here's what happens in simple steps:\n",
    "\n",
    "1. **Iteration**: The function iterates over each item in `docs`. Each item is referred to as `doc`.\n",
    "\n",
    "2. **Attribute Access**: For each `doc`, it accesses an attribute or property named `page_content`, which is a string holding the content of the document.\n",
    "\n",
    "3. **String Joining**: All the `page_content` strings from each document are combined into one single string. The individual content strings are separated by two newline characters (`\\n\\n`). This separation is used to ensure there is a blank line between the content of each document in the final output.\n",
    "\n",
    "4. **Return Value**: The resultant string, which is a combination of all the `page_content` strings separated by blank lines, is returned as the output of the function.\n",
    "\n",
    "In summary, `format_docs` function returns a single formatted string with each document's content separated by blank lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d8208-e9f3-46e4-b15c-61479b56b4ff",
   "metadata": {},
   "source": [
    "#### Take a look at the prompt we got from the LangChain hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa11d60f-7cb0-4ca4-81c5-004d77aaac57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the following line is not compatible with python 3.11.4\n",
    "#to install langchain-hub, you will have to use python 3.12.2 or superior\n",
    "\n",
    "#prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab7fd6-8d17-43e5-ab8d-3c5073afc54c",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 004-invoke-stream-batch.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 001-simple-rag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e1f91-62ba-446b-9c26-836784f11e00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
